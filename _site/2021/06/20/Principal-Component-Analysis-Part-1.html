

<!DOCTYPE html>
<html lang="en" class="h-100">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-2C7RJ7M0X5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-2C7RJ7M0X5');
    </script>
    <meta charset="UTF-8">
    <title>Tanveer Muttaqueen</title>
    <link rel = "icon" href ="/static/site-logo.png" type = "image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="This is my personal website. I write about my stories here. I also try to write
                                      about the things I am learning in my Blog">
    <meta name="keywords" content="Tanveer Muttaqueen Python Django Personal Blog Programming Contest ICPC">
    <meta name="author" content="Tanveer Muttaqueen">



        <!-- Bootstrap core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/custom.css" rel="stylesheet">
    <script src="/js/jquery-3.6.0.min.js"></script>
    <script src="/js/bootstrap.bundle.min.js"></script>

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }
      main > .container {
        padding: 60px 15px 0;
      }

      /** {*/
      /*    outline: purple solid 1px;*/
      /*}*/
    </style>



  </head>
  <body class="d-flex flex-column h-100 site-bg">

<header>

  <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">Tanveer Muttaqueen</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav me-auto mb-2 mb-md-0">
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blog_posts.html">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/experience.html">Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/programming_contests.html" >Contests</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/projects.html" >Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/static/Tanveer Muttaqueen Resume.pdf">My Resume</a>
          </li>
        </ul>

      </div>
    </div>
  </nav>
</header>
<!-- Begin page content -->
<main class="flex-shrink-0">
  <div class="container">
      <div class="row" style="padding-top: 30px">
          <div class="col-md-9">
              <div class="row">
    <div class="col-md-12">

        <h3 class="text-center"> PCA Using Singular Value Decomposition (Part 1) </h3>
        <p class="pt-4">Using Principal Component Analysis (PCA) we can change the basis of data points. Suppose, we have m data
        points each with n dimension. Then, using PCA we can map the data points using k ( k <= n ) unit vectors
        as the new basis. If k < n then we have reduced the dimension of the data points from n to k, may be with some
        data loss. In Machine Learning terms, each dimension is a feature vector. Thus we can use PCA to extract the
        important feature let say, most important subsets of feature that can explain 90% of data variance. </p>
        <p>
            Before continuing how PCA works and its mathematical explanation we need to have basic understanding of
            some linear algebra concepts.
            <ul>
                <li>
                    <h5> Eigenvalue, Eigenvector and Eigen decomposition </h5>
                    <p>
                        A matrix represents linear transformation of a vector. Let A be a linear transformation square matrix and v be
                        a vector, then y = Av is transformation of v using A. Linear transformation can have a rotation, then
                        a scaling then another rotation of a vector. Lets say v is vector on which transformation using A
                        doesn't cause it to rotate rather just makes it scale. Then, we can say v is a eigenvector of matrix
                        A and the scaling factor is a eigenvalue of matrix A.
                        <div class="important-note text-center">
                    <span class="math-eqn">Av = &lambda;v, where &lambda;&isin;R, A&isin;R<sup>nxn</sup> and v&isin;R<sup>nx1</sup> </span> <br>
                            Here v is a eigenvector and &lambda; is eigenvalue of matrix A.
                        </div>
                        If A is real symmetric matrix then it satisfies two properties of eigenvalue and eigenvector.
                        <ol>
                            <li>It has
                                n eigenvalues (not necessarily distinct) and one eigenvector for each eigenvalues. </li>
                            <li>These eigenvectors
                                are mutually orthogonal. </li>
                        </ol>
                        <div class="important-note">
                            So, we have <br>
                            <span class="math-eqn">
                                Av<sub>1</sub> = &lambda;<sub>1</sub>v<sub>1</sub> <br>
                                Av<sub>2</sub> = &lambda;<sub>2</sub>v<sub>2</sub> <br>
                                ...... <br>
                                Av<sub>n</sub> = &lambda;<sub>n</sub>v<sub>n</sub> <br>
                            </span>
                            Putting these all together <br>
                            <span class="math-eqn">
                                A [v<sub>1</sub> v<sub>2</sub> ... v<sub>n</sub>] = [v<sub>1</sub> v<sub>2</sub> ... v<sub>n</sub>] &Lambda;, <br>
                                Here &Lambda; is a diagonal matrix whose &Lambda;<sub>ii</sub> = &lambda;<sub>i</sub>
                                and v<sub>i</sub>'s are column vectors (also eigenvectors of A)
                                <br>
                                &rArr; AQ = Q&Lambda;,
                            </span><br>
                            For orthogonal matrices, we know that M<sup>-1</sup> = M<sup>T</sup>. So multiplying both
                            side by Q<sup>-1</sup>, we get<br>
                            <span class="math-eqn">
                                A = Q&Lambda;Q<sup>-1</sup> <br>
                                &rArr; A = Q&Lambda;Q<sup>T</sup>
                            </span>
                        </div>
                    Decomposing A with multiplication of three matrices using eigenvalues and eigenvectors is the
                    Eigendecomposition of matrix.

                    </p>

                </li>
                <li>
                    <h5> Singular value decomposition </h5>
                    <p>
                        Lets say A is m x n real matrix and v<sub>1</sub> and v<sub>2</sub> be two 1 x n real matrix which are
                        orthonormal. Then A transforms (ie Av) v<sub>i</sub> from R<sup>n</sup> dimension to R<sup>m</sup> dimension.
                        Lets the transformed vector be u<sub>1</sub> and u<sub>2</sub>. If we can find such orthonormal v<sub>i</sub>
                        such that their transformed vectors u<sub>i</sub>'s are also orthonormal, then we can write,
                        <div class="important-note">
                            <span class="math-eqn">
                                Av<sub>1</sub> = &sigma;<sub>1</sub>u<sub>1</sub> <br>
                                Av<sub>2</sub> = &sigma;<sub>2</sub>u<sub>2</sub> <br>
                                ...... <br>
                                Av<sub>n</sub> = &sigma;<sub>n</sub>u<sub>n</sub> <br>
                            </span>
                            Here &sigma;<sub>i</sub>'s are the scaling factor. Putting these all together, <br>
                            <span class="math-eqn">
                                A [v<sub>1</sub> v<sub>2</sub> ... v<sub>n</sub>] = [u<sub>1</sub> u<sub>2</sub> ... u<sub>n</sub>] &Sigma;, <br>
                                Here &Sigma; is a diagonal matrix whose &Sigma;<sub>ii</sub> = &sigma;<sub>i</sub> and v<sub>i</sub>'s, u<sub>i</sub>'s
                                are column vectors.<br>
                                 &rArr; AV = U&Sigma; <br>
                                 &rArr; A = U&Sigma;V<sup>T</sup>, Since U and V are orthonormal.
                            </span>
                        </div>
                        Here A is of dimension m x n, U is of dimension m x m, &Sigma; is of dimension m x n and V is
                        of dimension n x n. Now to find U and V we can use eigendecomposition of a matrix.
                        <div class="important-note">
                            <span class="math-eqn">
                                AA<sup>T</sup> = U&Sigma;V<sup>T</sup> V&Sigma;U<sup>T</sup> <br>
                                &rArr; AA<sup>T</sup> = U&Sigma;<sup>2</sup>U<sup>T</sup>
                            </span>
                        </div>
                        Now we can easily see that eigenvector matrix of AA<sup>T</sup> is the U in singular value decomposition.
                        <div class="important-note">
                            <span class="math-eqn">
                                A<sup>T</sup>A = V&Sigma;U<sup>T</sup>U&Sigma;V<sup>T</sup>  <br>
                                &rArr; A<sup>T</sup>A = V&Sigma;<sup>2</sup>V<sup>T</sup>
                            </span>
                        </div>
                        Now we can easily see that eigenvector matrix of A<sup>T</sup>A is the V in singular value decomposition. Also,
                        in both cases &Sigma;<sup>2</sup> = &Lambda; of eigenvalues. Since, &Lambda; is a diagonal matrix,
                        &Sigma;<sub>ii</sub> = sqrt(&Lambda;<sub>ii</sub>). &Sigma;<sub>ij</sub> = 0 if i != j.
                    </p>

                </li>
            </ul>

        </p>
        <p>
            Now how does PCA relates to these concepts? Lets say we want to reduce dimension of our feature vectors.
            We have m vectors with dimension n, given by a matrix A of dimension m x n. Lets assume the vectors are normalized
            such that mean of each dimension is 0. Now we want to reduce
            dimension from n to k ( k <= n ). Lets first consider k = 1. Let the basis vector of the new one dimensional
            coordinate system is x.
            <div class="text-center">
                <img src="/static/pca/pca-2dim-to-1dim.png" class="image-responsive" width="40%">
            </div>
            As we can see when reducing a 2 dim feature vector to 1 dim vector, among all the basis vector,
            the best basis vector has sum of distance from each point to the line is lowest. This is actually true for
            reducing any n dimension to k dimension. Sum of distance from each point to each basis vector is lowest.
            Then our dimension reduction loses lowest information. It is also easy to visualize this when reducing
            dimension from 3 to 2. From each point sum of distance to the 2 dimensional plane is lowest. Since in our
            feature vector their relative position with each other conveys the information, not how they are in n
            dimensional plane. Thus for a specific basis vector our goal is to reduce the distance from a point to this vector.
            In one dimensional case,
            <div class="important-note">
                Let the point be r. Our basis vector is x, distance vector from r to x is d and projection of r on x is p then,
                <div class="math-eqn">
                    |r|<sup>2</sup> = |d|<sup>2</sup> + |p|<sup>2</sup>
                </div>
            </div>
                But |r|<sup>2</sup> is fixed for a point. Thus minimizing |d| is equivalent to  maximizing |p|, ie
                the projection vector. This is actually true for any dimension, not for just 2 dimension. Here, normalizing
                the points to have mean 0 is important. Generalizing, If basis vector x is a unit vector and a is a point, Then,
            <div class="important-note">
                <div class="math-eqn">
                    |p| = ax, a have dimension 1xn and x have dimension nx1.
                </div>
                    In matrix form for each point,
                <div class="math-eqn">
                    P = Ax, here dim(P) = mx1, dim(A) = mxn and dim(x) = nx1.
                </div>
                P<sub>i1</sub> represent the projection of vector A<sub>i</sub> on x. Our target is to maximize
                Sum(P<sub>i1</sub><sup>2</sup>) for i = 1 to m. ie,
                <div class="math-eqn">
                    Find such x that |P| is maximized <br>
                    But max(|P|) = max(|Ax|), where x&isin;R<sup>n</sup>
                </div>
            </div>
            So we need to maximize |Ax|. Since Ax is a column vector we can write,
            <div class="important-note">
                <div class="math-eqn">
                    |Ax| = (Ax)<sup>T</sup>Ax = x<sup>T</sup>A<sup>T</sup>Ax
                </div>
            </div>
            Let v<sub>1</sub>, . . . , v<sub>n</sub> be an orthonormal basis of R<sup>n</sup> consisting of eigen vectors of
            A<sup>T</sup>A. With eigen values  &lambda;<sub>1</sub> >=  &lambda;<sub>2</sub> >= ..... >=  &lambda;<sub>r</sub> >= 0 >= .. >= 0.
            By the theory of singular value decomposition of A we know &sigma;<sub>i</sub><sup>2</sup> = &lambda;<sub>i</sub>. So
            we can write,
            <div class="important-note">
                <div class="math-eqn">
                     x = c<sub>1</sub>v<sub>1</sub> + c<sub>2</sub>v<sub>2</sub> + ... + c<sub>n</sub>v<sub>n</sub>.
                     Where, c<sub>i</sub>&isin; R and c<sub>1</sub><sup>2</sup> + .... c<sub>n</sub><sup>2</sup> <= 1, Since
                     x is a unit vector.
                </div>
            </div>
            Now using the properties of eigenvector, we can write,
            <div class="important-note">
                <div class="math-eqn">
                    A<sup>T</sup>Av<sub>1</sub> = &lambda;<sub>1</sub>v<sub>1</sub> <br>
                    Multiplying both side left by v<sub>1</sub><sup>T</sup> we get <br>
                    v<sub>1</sub><sup>T</sup>A<sup>T</sup>Av<sub>1</sub> = v<sub>1</sub><sup>T</sup>&lambda;<sub>1</sub>v<sub>1</sub> <br>
                    &rArr; v<sub>1</sub><sup>T</sup>A<sup>T</sup>Av<sub>1</sub> = &lambda;<sub>1</sub>, Since v<sub>i</sub>'s are unit vectors,
                    thus v<sub>1</sub><sup>T</sup>v<sub>1</sub> = 1
                </div>
            </div>
            Next, Putting the value of x in  x<sup>T</sup>A<sup>T</sup>Ax, we get
            <div class="important-note">
                <div class="math-eqn">
                    x<sup>T</sup>A<sup>T</sup>Ax = c<sub>1</sub><sup>2</sup>&lambda;<sub>1</sub> + .... + c<sub>n</sub><sup>2</sup>&lambda;<sub>n</sub>
                    <= &lambda;<sub>1</sub>( c<sub>1</sub><sup>2</sup> + .... + c<sub>n</sub><sup>2</sup>)
                    <= &lambda;<sub>1</sub> <br>
                    Equality holds if, x = v<sub>1</sub>.
                </div>
            </div>
            Thus |Ax| is maximized if x = v<sub>1</sub>. Here v<sub>1</sub> is the corresponding eigenvector of highest
            eigenvalue of A<sup>T</sup>A. So to reduce dimension of feature vector from n to k we can take the maximum k
            eigenvalue-eigenvector pair of A<sup>T</sup>A as basis vector. Interestingly, A<sup>T</sup>A is the co variance
            of A since they have mean value of 0.
        </p>


    </div>
</div>

          </div>
          <div class="col-md-3 left-differentiator">
              <ul class="list-group list-group-flush"> <h5> My Blog Posts </h5>
  <li class="list-group-item site-bg"> No Post yet </li>
</ul>
          </div>
      </div>

  </div>
</main>

<footer class="footer mt-auto py-3 bg-light-yellow">
  <div class="container">
    <span class="text-muted inline-text-center">
      &copy;2021 Tanveer Muttaqueen | Powered By: Github Pages
    </span>
  </div>
</footer>

  </body>
</html>
